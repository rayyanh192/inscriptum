"""
Training loop with feedback to prove self-learning.

Flow:
1. Process batch of emails ‚Üí make decisions
2. Review decisions ‚Üí apply corrections
3. Re-process same batch ‚Üí see improvement
4. Track metrics in Weave
"""

import asyncio
import sys
import weave
from dotenv import load_dotenv
load_dotenv('agent/.env')

# Add parent to path
sys.path.insert(0, '.')

# Import after path setup
import firebase_admin
from firebase_admin import credentials, firestore

# Initialize Firebase
if not firebase_admin._apps:
    cred = credentials.Certificate('convo/firebase-service-account.json')
    firebase_admin.initialize_app(cred)

db = firestore.client()

# Import agent functions
from agent.agent import process_email
from agent.feedback import record_feedback, process_feedback_for_learning

@weave.op()
async def process_batch(batch_size: int = 30):
    """Process a batch of emails and make decisions."""
    
    print(f"\n{'='*60}")
    print(f"üìß PROCESSING BATCH OF {batch_size} EMAILS")
    print(f"{'='*60}\n")
    
    # Get unprocessed emails
    emails = []
    docs = db.collection('emails').limit(batch_size).stream()
    for doc in docs:
        data = doc.to_dict()
        data['id'] = doc.id
        emails.append(data)
    
    print(f"Found {len(emails)} emails to process\n")
    
    decisions_made = []
    confidence_scores = []
    
    for i, email in enumerate(emails, 1):
        print(f"[{i}/{len(emails)}] Processing: {email.get('subject', 'No subject')[:50]}...")
        
        try:
            result = await process_email(email)
            
            if result and result.get('decision'):
                decision = result['decision']
                decisions_made.append({
                    'email_id': email['id'],
                    'subject': email.get('subject', ''),
                    'from': email.get('from', ''),
                    'action': decision.get('action'),
                    'confidence': decision.get('confidence', 0),
                    'reasoning': decision.get('reasoning', '')
                })
                confidence_scores.append(decision.get('confidence', 0))
                
                print(f"  ‚úÖ Action: {decision.get('action')} (confidence: {decision.get('confidence', 0):.2f})")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error: {e}")
    
    # Calculate metrics
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
    
    print(f"\n{'='*60}")
    print(f"‚ú® BATCH COMPLETE")
    print(f"{'='*60}")
    print(f"Decisions made: {len(decisions_made)}")
    print(f"Average confidence: {avg_confidence:.2f}")
    print(f"\nDecisions stored in: agent_decisions/")
    
    return {
        "decisions": decisions_made,
        "total": len(decisions_made),
        "avg_confidence": avg_confidence
    }


@weave.op()
async def review_and_correct(decisions: list):
    """Interactive review of decisions for corrections."""
    
    print(f"\n{'='*60}")
    print(f"üîç REVIEWING DECISIONS")
    print(f"{'='*60}\n")
    
    corrections_applied = 0
    
    print("Review recent decisions and provide corrections:")
    print("(This will be used to train the agent)\n")
    
    # Show sample of decisions
    sample_size = min(10, len(decisions))
    print(f"Showing {sample_size} decisions for review:\n")
    
    for i, decision in enumerate(decisions[:sample_size], 1):
        print(f"{i}. Email: {decision['subject'][:60]}")
        print(f"   From: {decision['from'][:40]}")
        print(f"   Agent decided: {decision['action']} (confidence: {decision['confidence']:.2f})")
        print(f"   Reasoning: {decision['reasoning'][:80]}...")
        print()
    
    print("\nüí° To apply corrections, you would:")
    print("   1. Mark which decisions were wrong")
    print("   2. Specify correct action")
    print("   3. Agent learns pattern from correction")
    print("\n   Example: 'Decision #3 should be archive, not reply'")
    
    # For now, simulate some corrections
    print("\nüîÑ Simulating corrections for demonstration...\n")
    
    # Simulate 3 corrections
    simulated_corrections = [
        {
            "email_id": decisions[0]['email_id'] if decisions else None,
            "wrong_action": decisions[0]['action'] if decisions else "reply",
            "correct_action": "archive",
            "feedback": "Newsletter emails should be archived, not replied to"
        },
        {
            "email_id": decisions[2]['email_id'] if len(decisions) > 2 else None,
            "wrong_action": decisions[2]['action'] if len(decisions) > 2 else "ignore",
            "correct_action": "star",
            "feedback": "Emails from personal friends should be starred"
        },
        {
            "email_id": decisions[5]['email_id'] if len(decisions) > 5 else None,
            "wrong_action": decisions[5]['action'] if len(decisions) > 5 else "reply",
            "correct_action": "ignore",
            "feedback": "Marketing emails should be ignored"
        }
    ]
    
    for correction in simulated_corrections:
        if correction['email_id']:
            print(f"‚úèÔ∏è  Correction: {correction['wrong_action']} ‚Üí {correction['correct_action']}")
            print(f"   Reason: {correction['feedback']}")
            
            # Apply feedback via record_feedback
            feedback_result = await record_feedback(
                decision_id=correction['email_id'],
                feedback_type='action_wrong',
                feedback_data={
                    'correct_action': correction['correct_action'],
                    'agent_action': correction['wrong_action'],
                    'user_note': correction['feedback']
                },
                db=db
            )
            
            # Process for learning
            await process_feedback_for_learning(feedback_result, db)
            
            corrections_applied += 1
            print(f"   ‚úÖ Pattern learned!\n")
    
    print(f"{'='*60}")
    print(f"üìö LEARNING COMPLETE")
    print(f"{'='*60}")
    print(f"Corrections applied: {corrections_applied}")
    print(f"Agent has updated its patterns\n")
    
    return corrections_applied


@weave.op()
async def measure_improvement(original_decisions: list):
    """Re-process same emails to measure improvement."""
    
    print(f"\n{'='*60}")
    print(f"üìä MEASURING IMPROVEMENT")
    print(f"{'='*60}\n")
    
    print("Re-processing same emails with updated patterns...\n")
    
    improved_count = 0
    confidence_increases = []
    
    for decision in original_decisions[:10]:  # Sample
        email_id = decision['email_id']
        
        # Get email from Firebase
        email_doc = db.collection('emails').document(email_id).get()
        if not email_doc.exists:
            continue
        
        email_data = email_doc.to_dict()
        email_data['id'] = email_id
        
        # Re-process
        try:
            new_result = await process_email(email_data)
            if new_result and new_result.get('decision'):
                new_decision = new_result['decision']
                old_confidence = decision['confidence']
                new_confidence = new_decision.get('confidence', 0)
                
                confidence_change = new_confidence - old_confidence
                confidence_increases.append(confidence_change)
                
                if new_confidence > old_confidence:
                    improved_count += 1
                    print(f"‚úÖ Improved: {email_data.get('subject', '')[:50]}")
                    print(f"   Confidence: {old_confidence:.2f} ‚Üí {new_confidence:.2f} (+{confidence_change:.2f})")
                else:
                    print(f"‚Üí  Unchanged: {email_data.get('subject', '')[:50]}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error re-processing: {e}")
    
    # Calculate improvement metrics
    avg_improvement = sum(confidence_increases) / len(confidence_increases) if confidence_increases else 0
    improvement_rate = (improved_count / len(original_decisions[:10])) * 100 if original_decisions else 0
    
    print(f"\n{'='*60}")
    print(f"üìà RESULTS")
    print(f"{'='*60}")
    print(f"Emails with improved confidence: {improved_count}/{len(original_decisions[:10])}")
    print(f"Improvement rate: {improvement_rate:.1f}%")
    print(f"Average confidence increase: +{avg_improvement:.2f}")
    
    # Get pattern count
    patterns_doc = db.collection('learned_patterns').document('importance').get()
    if patterns_doc.exists:
        patterns = patterns_doc.to_dict().get('rules', [])
        print(f"Total patterns learned: {len(patterns)}")
    
    print(f"\n‚ú® Self-learning demonstrated!")
    print(f"   Check Weave dashboard for detailed traces:")
    print(f"   https://wandb.ai/inscriptum85-inscriptum/email-agent/weave\n")
    
    return {
        "improved_count": improved_count,
        "improvement_rate": improvement_rate,
        "avg_confidence_increase": avg_improvement
    }


@weave.op()
async def training_loop():
    """Complete training loop: process ‚Üí correct ‚Üí measure."""
    
    print("\n" + "="*60)
    print("üöÄ STARTING SELF-LEARNING TRAINING LOOP")
    print("="*60)
    print("\nThis will:")
    print("  1. Process a batch of emails")
    print("  2. Apply feedback corrections")
    print("  3. Re-process to measure improvement")
    print("  4. Track everything in Weave")
    print()
    
    # Step 1: Initial processing
    batch_result = await process_batch(batch_size=30)
    
    # Step 2: Review and correct
    corrections = await review_and_correct(batch_result['decisions'])
    
    # Step 3: Measure improvement
    improvement = await measure_improvement(batch_result['decisions'])
    
    print("\n" + "="*60)
    print("üéØ TRAINING LOOP COMPLETE")
    print("="*60)
    print(f"\nSummary:")
    print(f"  - Initial decisions: {batch_result['total']}")
    print(f"  - Initial confidence: {batch_result['avg_confidence']:.2f}")
    print(f"  - Corrections applied: {corrections}")
    print(f"  - Improvement rate: {improvement['improvement_rate']:.1f}%")
    print(f"  - Avg confidence boost: +{improvement['avg_confidence_increase']:.2f}")
    print(f"\n‚úÖ Agent has learned and improved!")
    print(f"   View traces: https://wandb.ai/inscriptum85-inscriptum/email-agent/weave\n")


if __name__ == "__main__":
    weave.init('email-agent')
    asyncio.run(training_loop())
